---
layout: project
permalink: /:title/
category: projects

meta:
  keywords: "Jekyll, Ludo, AI"

project:
  title: "AI for Ludo"
  type: "AI"
  url: "https://github.com/arnolds/pineapple"
  logo: "/assets/images/projects/aquapineapple/logo.png"
  tech: "Java, AI"

agency:
  title: "Aqua Pineapple Co"
  url: "https://github.com/arnolds/pineapple"
  year: "2010"

images:
  - image:
    url: "/assets/images/projects/aquapineapple/devices.jpg"
    alt: "Aqua Pineapple website on tablet, mobile and desktop"
  - image:
    url: "/assets/images/projects/aquapineapple/desktop.jpg"
    alt: "Aqua Pineapple website on a desktop device"
  - image:
    url: "/assets/images/projects/aquapineapple/mobile.jpg"
    alt: "Aqua Pineapple website on a mobile device"
---
<p>Ludo is a board game that allows up to 4 players which originates from the Indian game Pachisi. Ludo provides an interesting problem from an AI perspective in that while its mostly a min-maxing technique to move one’s pieces in the most optimal manner, the other players’ involvement may sometimes not give ideal results. Then the key question would be - are there techniques better suited than a simple min-max approach? Can a random approach do equally good? Such questions were analyzed during this research oriented project where some modern AI techniques were tested for their success rate at playing the game.</p>
<p>The project involved using a digital version of Ludo game as the testbed. The testbed was provided through the “Modern AI in games” class at IT University of Copenhagen, Denmark and it’s LUDOPlayer class can be overloaded to create a custom player. In this case, 3 kinds of players were devised:</p>
<ul>
	<li><strong>Trained neural network player</strong> - This player would find the action with the highest desirability value and carry it out. The neural network was pre-trained with certain datasets obtained from how a min-maxing strategy player would play it.</li>
	<li><strong>Player trained through Temporal Difference</strong> - Again, a neural network would analyze the desirability of any move possible and carry out the one it would think best. However each move’s success/failure would be used to train the network. For example - if the move meant that the player’s piece got to home, then program tells the network to give that kind of output. However, if it lead to the piece being intercepted by another player, then network should not give that result.</li>
	<li><strong>Player evolved via Genetic Algorithm</strong> - The game’s controlling neural network is put through a genetic algorithm to find the configuration with the best average result history.</li>
</ul>